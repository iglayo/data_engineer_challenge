# Axpo — Advanced Analytics: Data Software Engineer Challenge

**Author:** Ignacio Layo González
**Role:** Data Engineer (Software) — technical challenge  
**Repository:** axpo_data_engineer_challenge

---

Proyecto para el reto: construir un workflow que **predice las siguientes 6 horas (T+1..T+6)** de demanda eléctrica a **resolución horaria**, partiendo del **último datapoint disponible en ESIOS (T)**.  
Este repo contiene la extracción (ETL), preprocesado, pipeline de features, entrenamiento de un modelo simple y la generación de predicciones (parquet) listas para entrega.

---

.
├── data/ # (ignored by git)
│ ├── raw/ # raw files (no versionados)
│ ├── processed/ # hourly, features, predictions, script generated
│ ├── predictions/ # predictions generated by the model
├── scripts/
│ ├── process_features.py # genera hourly y features desde raw
│ └── run_model.py # entrena y genera predictions T+1..T+6
├── src/
│ ├── config.py # rutas y configuración
│ ├── etl.py # fetch de ESIOS (no incluye clave en repo)
│ ├── features.py # feature engineering
│ └── model.py # entrenamiento y forecast recursivo
├── tests/ # pytest tests
├── requirements.txt
└── README.md

---

## Setup

1. Clone the repo:
```bash
git clone https://github.com/iglayo/data_engineer_challenge.git
cd axpo_data_engineer_challenge

2. Create and activate virtual environment (recommended):
Windows (PowerShell):
python -m venv .venv
.\.venv\Scripts\Activate.ps1

3. Install dependencies:
pip install -r requirements.txt

4. Export ESIOS token as env variable:
$Env:ESIOS_API_KEY="token in here"

---

## Cómo ejecutar
PowerShell
1. Extract data from ESIOS export to parquet format, running:
python run_pipeline.py
Downloads the last 21 days of demand and normalizes dates to UTC
Save the file to data/raw/1293_YYYY-MM-DD_YYYY-MM-DD.parquet, from the root of the project
If there is no API key or the API fails, it will attempt to load data/example_1293.csv

2. Generate time features
python .\scripts\process_features.py

Converts 5-minute data to hourly frequency.
Creates lags (t-1, t-24, t-168) and rolling stats (mean, std, median).
Adds temporal features (hour, day, month, sines/cosines).

Saves:
data/processed/raw_*.parquet (backup)
data/processed/hourly_*.parquet
data/processed/features_*.parquet

3. Train model and generate predictions T+1…T+6
python .\scripts\run_model.py

Splits the data temporally (train_val_split_time).
Trains a RandomForestRegressor on the time features.
Evaluates with MAE in validation.
Predicts the next 6 hours from the last observed timestamp (T).

Saves:
models/rf_model.joblib (trained model)
data/predictions/predictions_features_*.parquet

Final output format
datetime(UTC)	        target  prediction
2025-10-05 16:05:00+00	NaN	20320.30
2025-10-05 17:05:00+00	NaN	19704.88
2025-10-05 18:05:00+00	NaN	19671.39
...	...	...

datetime: Expected time (T+1..T+6).
target: NaN (because those values ​​don't exist yet).
prediction: Predicted value.


### Technical Decisions

The raw data file with data each5 minutes is kept for traceability and future experimentation. This allows for a higher-resolution model to be developed in the future (the current one has hourly resolution).
It is trained using an hourly view (hourly average): this maintains consistency with the lags, and the model is consistent with the training data.
Predictions are made from T, the latest ESIOS datapoint.
A Random Forest is used as a model for its application in time series.